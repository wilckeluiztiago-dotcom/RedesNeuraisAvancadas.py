# RedesNeuraisAvancadas

**Autor:** Luiz Tiago Wilcke

Uma biblioteca completa de redes neurais implementadas do zero em Python/NumPy. ContÃ©m 20+ arquiteturas diferentes.

## ğŸ§  Redes Neurais DisponÃ­veis

| # | Rede | DescriÃ§Ã£o |
|---|------|-----------|
| 1 | **Perceptron** | NeurÃ´nio artificial bÃ¡sico |
| 2 | **MLP** | Multi-Layer Perceptron |
| 3 | **CNN** | Rede Convolucional |
| 4 | **RNN** | Rede Recorrente |
| 5 | **LSTM** | Long Short-Term Memory |
| 6 | **GRU** | Gated Recurrent Unit |
| 7 | **Autoencoder** | CompressÃ£o/ReconstruÃ§Ã£o |
| 8 | **VAE** | Variational Autoencoder |
| 9 | **GAN** | Generative Adversarial Network |
| 10 | **Transformer** | Mecanismo de AtenÃ§Ã£o |
| 11 | **ResNet** | Rede Residual |
| 12 | **DenseNet** | Blocos Densos |
| 13 | **Hopfield** | MemÃ³ria Associativa |
| 14 | **Kohonen SOM** | Mapa Auto-OrganizÃ¡vel |
| 15 | **RBF** | Radial Basis Function |
| 16 | **Boltzmann** | MÃ¡quina de Boltzmann Restrita |
| 17 | **ESN** | Echo State Network |
| 18 | **Capsule** | Capsule Network |
| 19 | **GNN** | Graph Neural Network |
| 20 | **GAT** | Graph Attention Network |

## ğŸ“ Estrutura do Projeto

```
RedesNeuraisAvancadas/
â”œâ”€â”€ redes/
â”‚   â”œâ”€â”€ __init__.py      # ExportaÃ§Ãµes
â”‚   â”œâ”€â”€ base.py          # Classes fundamentais
â”‚   â”œâ”€â”€ ativacoes.py     # 12 funÃ§Ãµes de ativaÃ§Ã£o
â”‚   â”œâ”€â”€ otimizadores.py  # 8 otimizadores
â”‚   â”œâ”€â”€ perdas.py        # 10 funÃ§Ãµes de perda
â”‚   â”œâ”€â”€ mlp.py           # Perceptron, MLP
â”‚   â”œâ”€â”€ cnn.py           # CNN, Pooling, BatchNorm
â”‚   â”œâ”€â”€ lstm.py          # LSTM, GRU
â”‚   â”œâ”€â”€ autoencoder.py   # Autoencoder, VAE
â”‚   â”œâ”€â”€ gan.py           # GAN
â”‚   â”œâ”€â”€ transformer.py   # Transformer, AtenÃ§Ã£o
â”‚   â”œâ”€â”€ especiais.py     # ResNet, Hopfield, SOM, RBF, ESN
â”‚   â””â”€â”€ grafos.py        # Capsule, GNN, GAT
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ dados.py         # GeraÃ§Ã£o de dados
â”‚   â””â”€â”€ visualizacao.py  # GrÃ¡ficos
â”œâ”€â”€ principal.py         # Demo completa
â””â”€â”€ README.md
```

## ğŸš€ InÃ­cio RÃ¡pido

### InstalaÃ§Ã£o

```bash
# Clone o repositÃ³rio
cd RedesNeuraisAvancadas

# Apenas NumPy Ã© necessÃ¡rio
pip install numpy
```

### Exemplo: MLP para ClassificaÃ§Ã£o

```python
from redes import MLP, Adam, EntropiaCruzadaCategorica
from utils import gerar_dados_classificacao, dividir_dados, acuracia

# Gera dados
X, y = gerar_dados_classificacao(n_amostras=1000, n_classes=3)
X_treino, X_teste, y_treino, y_teste = dividir_dados(X, y)

# Cria modelo
modelo = MLP.criar_classificador(entrada=10, classes=3, ocultas=[64, 32])
modelo.compilar(Adam(taxa_aprendizado=0.01), EntropiaCruzadaCategorica())

# Treina
modelo.treinar(X_treino, y_treino, epocas=100, tamanho_lote=32)

# Avalia
pred = modelo.prever(X_teste)
print(f"AcurÃ¡cia: {acuracia(y_teste, pred):.2%}")
```

### Exemplo: GAN

```python
from redes import GAN
import numpy as np

# Dados reais
dados = np.random.randn(1000, 10) * 2 + 5

# Treina GAN
gan = GAN(dim_latente=20, dim_dados=10)
gan.treinar(dados, epocas=100)

# Gera novas amostras
novas = gan.gerar(100)
```

### Exemplo: Rede de Hopfield

```python
from redes import RedeHopfield
import numpy as np

# PadrÃµes para memorizar
padroes = np.array([[1, -1, 1, -1], [-1, 1, -1, 1]])

rede = RedeHopfield(tamanho=4)
rede.treinar(padroes)

# Recupera padrÃ£o corrompido
entrada = np.array([1, -1, 1, 1])  # Com ruÃ­do
recuperado = rede.recuperar(entrada)
```

## ğŸ› ï¸ Componentes

### FunÃ§Ãµes de AtivaÃ§Ã£o
- ReLU, LeakyReLU, ELU, SELU
- Sigmoide, Tanh, Softmax
- Swish, GELU, Mish

### Otimizadores
- SGD (com momentum)
- Adam, AdamW, NAdam, RAdam
- RMSprop, Adagrad, Adadelta

### FunÃ§Ãµes de Perda
- MSE, MAE, Huber
- Entropia Cruzada (binÃ¡ria, categÃ³rica)
- KL Divergence, Hinge, Focal Loss

## â–¶ï¸ Executar Demo

```bash
python principal.py
```

## ğŸ“‹ Requisitos

- Python 3.8+
- NumPy
- Matplotlib (opcional, para visualizaÃ§Ã£o)

## ğŸ“ LicenÃ§a

MIT License

## ğŸ‘¤ Autor

**Luiz Tiago Wilcke**

---

*Biblioteca desenvolvida para fins educacionais, demonstrando implementaÃ§Ã£o de redes neurais do zero.*
